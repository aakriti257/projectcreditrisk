{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "820d282c-24b3-41c5-8274-1d9fd8dd9db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ---------------- Initialize Spark ----------------\n",
    "spark = SparkSession.builder.appName(\"CreditRisk_GoldLayer\").getOrCreate()\n",
    "\n",
    "# ---------------- Silver Layer Details ----------------\n",
    "silver_catalog = \"silver1\"\n",
    "silver_schema = \"credit_data\"\n",
    "silver_table = \"credit_silver\"\n",
    "\n",
    "# ---------------- Gold Layer Details ----------------\n",
    "gold_catalog = \"gold1\"\n",
    "gold_schema = \"analytics\"\n",
    "gold_table = \"risk_features\"\n",
    "\n",
    "# ---------------- Step 1: Load Silver Data ----------------\n",
    "df_silver = spark.table(f\"{silver_catalog}.{silver_schema}.{silver_table}\")\n",
    "\n",
    "print(\"Schema of Silver Data:\")\n",
    "df_silver.printSchema()\n",
    "\n",
    "# ---------------- Step 2: Feature Engineering ----------------\n",
    "# Risk Score = (loan_amnt / person_income) * risk factor (double weight if defaulted)\n",
    "df_gold = (\n",
    "    df_silver\n",
    "    .withColumn(\n",
    "        \"risk_score\",\n",
    "        (F.col(\"loan_amnt\") / (F.col(\"person_income\") + F.lit(1))) *\n",
    "        (F.when(F.col(\"loan_status\") == 1, 2).otherwise(1))\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------------- Step 3: Aggregated Risk Metrics ----------------\n",
    "# Group by multiple dimensions\n",
    "group_cols = [\"person_age\", \"loan_intent\", \"loan_grade\", \"person_home_ownership\"]\n",
    "\n",
    "df_aggregated = (\n",
    "    df_gold.groupBy(*group_cols)\n",
    "    .agg(\n",
    "        F.avg(\"risk_score\").alias(\"avg_risk_score\"),\n",
    "        F.avg(\"person_income\").alias(\"avg_income\"),\n",
    "        F.avg(\"loan_amnt\").alias(\"avg_loan_amount\"),\n",
    "        F.sum(\"loan_status\").alias(\"total_defaults\"),\n",
    "        F.count(\"*\").alias(\"customer_count\"),\n",
    "        (F.sum(\"loan_status\") / F.count(\"*\")).alias(\"default_rate\")\n",
    "    )\n",
    "    .orderBy(\"person_age\", \"loan_intent\", \"loan_grade\")\n",
    ")\n",
    "\n",
    "# ---------------- Step 4: Create Gold Schema ----------------\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {gold_catalog}.{gold_schema}\")\n",
    "\n",
    "# ---------------- Step 5: Save Gold Table ----------------\n",
    "(\n",
    "    df_aggregated.write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(f\"{gold_catalog}.{gold_schema}.{gold_table}\")\n",
    ")\n",
    "\n",
    "print(f\"âœ… Gold Layer Table Created: {gold_catalog}.{gold_schema}.{gold_table}\")\n",
    "\n",
    "# ---------------- Step 6: Verify ----------------\n",
    "df_gold_sample = spark.sql(f\"SELECT * FROM {gold_catalog}.{gold_schema}.{gold_table} LIMIT 10\")\n",
    "df_gold_sample.show(truncate=False)\n",
    "# Load Gold Table\n",
    "df_gold_final = spark.table(f\"{gold_catalog}.{gold_schema}.{gold_table}\")\n",
    "\n",
    "# Display Data (Databricks interactive table)\n",
    "display(df_gold_final)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "goldfinal",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
